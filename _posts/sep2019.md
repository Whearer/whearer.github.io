
Authors: Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok

Arxiv Link: https://arxiv.org/pdf/1707.07397.pdf

Executive Summary: Adversarial examples can currently be created relatively easily by perturbing an image by a particular amount specific to the classifier. However, prior work has shown that the adversarial example generated using this technique lose their adversarial nature once transformed or altered in any way, which often happens in the real world. The authors introduce a new approach, the Expectation Over Transformation (EOT) algorithm, that is able to produce adversarial examples that remain adversarial after being transformed under any transformation T, facilitating the creation of 3D objects that are misclassified from all angles. This implies that these methods are, in fact, a concrete threat to real-world systems.

Notable Details: The authors’ new approach, Expectation Over Transformation (EOT), is different because rather than optimizing the log-likelihood of a single example, EOT uses a chosen distribution T of transformation functions t taking an input x generated by the adversary to the “true” input t(x) perceived by the classifier. In practice, we can have T model random rotation, translation, or addition of noise.
