---
layout: post
title:  "Reading List of Papers"
date:   2019-09-18 14:06:05
categories: Papers
excerpt: Some interesting papers
---
# key: Active Learning, Multi-Objective Optimisation

[e-PAL: An Active Learning Approach to the Multi-Objective Optimization Problem](http://jmlr.org/papers/volume17/15-047/15-047.pdf)
 
[Active Learning for Multi-Objective Optimizationm](http://proceedings.mlr.press/v28/zuluaga13.pdf)


The implementation code can be found [here](http://www.spiral.net/software/pal/epal_public.zip).


[Active learning of Pareto fronts](http://eprints.biblio.unitn.it/4087/1/ALP_tech_rep_with_cover.pdf)


[Active learning of Pareto fronts with disconnected feasible](http://disi.unitn.it/~passerini/papers/mic2013alp.pdf)


Title: Synthesizing robust adversarial examples 

Authors: Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok

Arxiv Link: https://arxiv.org/pdf/1707.07397.pdf

---

# Algorithm parameter configuration

On continuous problems, it can select the optimal algorithm parameter.
[Per instance algorithm configuration of CMA-ES with limited budget](https://hal.inria.fr/hal-01613753/file/GECCO2017_nacimbelkhir.pdf)
and [slides](http://211.65.106.2/cache/4/03/www.coseal.net/a1f612522f81001634097d53c9b46620/slidesSchoenauerCOSEAL2017.pdf)

A more completed method is presented on SAT, TSP and MIP problems.
[Algorithm runtime prediction: Methods & evaluation](https://www.sciencedirect.com/science/article/pii/S0004370213001082)

[Sequential Model-Based Optimization for General Algorithm Configuration](https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf)
This paper propose the SMAC algorithm, a random forest based Bayesian optimization algorithm.

---
# Reinforcement Learning

First of all, a list of resource for RL including courses, papers/surveys, benchmarks/testbeds [Resources for Deep Reinforcement Learning](https://medium.com/@yuxili/resources-for-deep-reinforcement-learning-a5fdf2dc730f)

OpenAI tutorial [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/index.html)

[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf)

[Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/pdf/1802.09477.pdf)

[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)

[Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/pdf/1710.10044.pdf)

[Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning](https://arxiv.org/pdf/1803.00101.pdf)

[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf)

# key: Deep learning as Bayesian inference

[Dropout as a Bayesian Approximation]

# key: High-Dimensional

[Dropout as a Bayesian Approximation]

---
# key: Bandits

[High-Dimensional Gaussian Process Bandits](https://las.inf.ethz.ch/files/djolonga13high-long.pdf)

[Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards](http://papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf)


---
# key: Natural Gradient

[Natural Gradient Deep Q-learning](https://arxiv.org/pdf/1803.07482v2.pdf)



# Clustering

[Efficient Parameter-free Clustering Using First Neighbor Relations](https://arxiv.org/abs/1902.11266)
